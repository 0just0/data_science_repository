{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Наивный Байесовский классификатор\n",
    "\n",
    "Наивная Байесовская классификация - один из самых элегантных практически используемых алгоритмов машинного обучения. Он является мощным и эффективным инструментов по качеству результата.\n",
    "\n",
    "* Устойчив к нерелевантным признакам, которые просто игнорирует\n",
    "* Быстро обучается и быстро возвращает предсказание\n",
    "* Потребляет относительно небольшое число ресурсов\n",
    "\n",
    "В чем же наивность?\n",
    "\n",
    "Наивность относится к предположению, необходимому для оптимальной работы классификатора. Состоит оно в том, что признаки не влияют друг на друга. В реальности такое бывает крайне редко, но на практике верность этого алгоритма достаточно выскоа, даже если предположение о независимости признаков не оправдывается.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теорема Байеса\n",
    "\n",
    "По сути, наивная Байесовская классификация - не что иное, как отслеживание того, какой признак о каком классе свидетельствует. Способ проектирования признаков определяет модель, используемую для обучения.\n",
    "\n",
    "Например, в **модели Бернулли** допускаются только буоевские признаки (встречается слово один или несколько раз не имеет значения). В мультиномиальной модели признаками являются счетчики слов.\n",
    "\n",
    "Пока рассмотрим модель Бернулли, чтобы объяснить как наивный Байесовский классификатор используется для анализа эмоциональной окраски текста. А для обучения и настройки реальных классификаторов (позже) перейдем на **мультиномиальную модель**.\n",
    "\n",
    "Итак, пусть:\n",
    "* $C$ - Класс твита (положительный или отрицательный);\n",
    "* $F_1$ - В твите хотя бы раз встречается слово awesome;\n",
    "* $F_2$ - В твите хоть раз встречается слово crazy\n",
    "\n",
    "В ходе обучения мы построили наивную Байесовскую модель, которая возвращает вероятность класса $C$, если известны признаки $F_1$ и $F_2$. Эта вероятность записывается в виде $\\Pr(C\\mid F_1, F_2)$.\n",
    "\n",
    "Поскольку мы не можем оценить  $\\Pr(C\\mid F_1, F_2)$ непосредственно, то применим формулу, изобретенную Байесом:\n",
    " $$\\Pr(B)\\times\\Pr(B\\mid A) = \\Pr(A)\\times\\Pr(A\\mid B)$$ или $$ \\Pr(B \\mid A) = \\frac{\\Pr(A)\\times\\Pr(A \\mid B)}{\\Pr(B)}$$\n",
    " \n",
    "Если считать, что $A$ - событие.ю состоящее во вхождении обоих слов awesome и crazy, а $B$ - принадлежность твита классу $C$, то получится формула, которая впоследствии может помочь нам вычислить вероятность принадлежности образца к указанному классу:\n",
    "\n",
    "$$\\Pr(F_1, F_2) \\times \\Pr(C\\mid F_1, F_2) = \\Pr(C)\\times\\Pr(F_1, F_2\\mid C).$$\n",
    "\n",
    "Это позволяет выразить $\\Pr(C\\mid F_1, F_2)$ через другие вероятности:\n",
    "\n",
    "$$\\Pr(C\\mid F_1, F_2) = \\frac{\\Pr(C)\\times \\Pr(F_1, F_2\\mid C)}{\\Pr(F_1, F_2)}$$\n",
    "\n",
    "Можно и записать в таком виде:\n",
    "\n",
    "$$prior = \\frac{posterior \\times likelihood}{evidence}$$\n",
    "\n",
    "$prior$ м $evidence$ найти легко:\n",
    "* $P(C)$ - априорная вероятность класса без каких-либо знаний о данных. Оценить её можно напрямую подсчитав долю обучающих примеров, принадлежащих данному классу.\n",
    "* $P(F_1, F_2)$ - свидетельство, или вероятность одновременного наличия признаков $F_1$ и  $F_2$.\n",
    "\n",
    "Нетривиальная часть - вычисление правдоподобия (likelihood) $\\Pr(F_1, F_2\\mid C)$. Эта величина говорит о том, насколько вероятно увидеть признаки $F_1$ и  $F_2$, если мы знаем, что образец принадлежит классу $C$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предположение о наивности\n",
    "Из теории вероятностей:\n",
    "$$ \\Pr(F_1, F_2 \\mid C) = \\Pr(F_1 \\mid C) \\times \\Pr(F_2 \\mid C, F_1).$$\n",
    "\n",
    "Сама по себе формула мало что дает, поскольку мы заменяем одну трудную задачу - поиск $\\Pr(F_1, F_2 \\mid C)$ другой, не менее трудной - оценка $ \\Pr(F_2 \\mid C, F_1)$.\n",
    "\n",
    "Однако, если наивно предположить, что $F_1$ и $F_2$ независимы, то $ \\Pr(F_2 \\mid C, F_1)$ сводится к $ \\Pr(F_2 \\mid C)$ и мы можем записать:\n",
    "$$\\Pr(F_1, F_2 \\mid C) = \\Pr(F_1 \\mid C)\\times\\Pr(F_2 \\mid C).$$\n",
    "\n",
    "Собирая всё вместе, получаем простую формулу:\n",
    "$$\\Pr(C\\mid F_1, F_2) = \\frac{\\Pr(C)\\times\\Pr(F_1 \\mid C) \\Pr(F_2 \\mid C)}{\\Pr(F_1, F_2)}$$.\n",
    "\n",
    "Любопытная вещь: хотя теоретически неправильно выдвигать произвольные предположения под настроение, в данном случае такой подход на удивление хорошо работает в реальных задачах.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование наивного Байесовского алгоритма для классификации\n",
    "\n",
    "Итак, получив новый твит, мы должны вычислить вероятности:\n",
    "\n",
    "$$\\Pr(C = \\text{\"pos\"} \\mid F_1, F_2) = \\frac{\\Pr(C = \\text{\"pos\"})\\times\\Pr(F_1 \\mid \\text{C = \"pos\"}) \\times \\Pr(F_2 \\mid C = \\text{\"pos\"})}{\\Pr(F_1, F_2)}$$.\n",
    "\n",
    "$$\\Pr(C = \\text{\"neg\"} \\mid F_1, F_2) = \\frac{\\Pr(C = \\text{\"neg\"} )\\times\\Pr(F_1 \\mid C = \\text{\"neg\"} ) \\times\\Pr(F_2 \\mid C = \\text{\"pos\"} )}{\\Pr(F_1, F_2)}$$.\n",
    "\n",
    "А затем выбрать класс $C_{best}$ с наибольшей вероятностью.\n",
    "\n",
    "\n",
    "Поскольку для обоих классов знаменатель одинаковый мы его можем просто игнорировать - предсказанный класс от этого не изменится.\n",
    "\n",
    "\n",
    "Однако, стоит отметить, что реальные вероятности больше не вычисляются Вместо этого оценивается, какой класс более правдоподобен, по имеющимся свидетельствам. Это еще одна причина устойчивости наивного байесовского классификатора: его интересуют не столько истинные вероятности, сколько информация о том, какой клсс правдоподобнее.\n",
    "\n",
    "Короче говоря, можно написать:\n",
    "$$C_{best} = \\arg\\max_{c\\in C} \\Pr(C=c)\\times\\Pr(F_1\\mid C=c)\\times\\Pr(F_2\\mid C=c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Говоря, что мы вычисляем часть после $\\arg\\max$ для всех классов и возвращаем тот класс, для которого получилось наибольшее значение.\n",
    "\n",
    "Проиллюстрируем, чтобы понаблюдать за работой наивного байесовского алгоритма. Сделаем предположение, что Twitter разрешает употреблять только два слова: awesome и crazy и что мы уже вручную проклассифицировали несколько твитов:\n",
    "\n",
    "| Твит | Класс \n",
    "| :-:  | :-:\n",
    "|awesome | pos\n",
    "|awesome | pos\n",
    "|awesome crazy | pos\n",
    "|crazy | pos\n",
    "|crazy | neg\n",
    "|crazy | neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Твит crazy получил как отрицательную, так и положительную оценку (моделируем реальныую речь: \"балдеть от футбола\" и \"дурацкий идиот\").\n",
    "Всего шесть твитов - 4 pos и 2 neg, поэтому получаем априорные вероятности:\n",
    "$$\\Pr(C=pos) = \\frac{4}{6} ~ 0.67$$\n",
    "$$\\Pr(C=neg) = \\frac{2}{6} ~ 0.33$$\n",
    "\n",
    "Это означает, что ничего не зная о самом твите, разумно предположить что он положительный.\n",
    "\n",
    "Пока отсутсвует вычисление $\\Pr(F_1\\mid C=c)$ и $\\Pr(F_2\\mid C=c)$ - вероятностей признаков $F_1$ и $F_2$ при условии класса $C$. Они вычисляются как количество твитов, в которых встречался отдельный признак, поделенное на количество твитов помеченных классом $C$.\n",
    "\n",
    "Вероятность встретить awesome, если известно что класс положительный:\n",
    "$$\\Pr(F_1 = 1 \\mid C=pos) = \\frac{\\text{число положительных твитов, содержащих слово awesome}}{\\text{число всех положительных твитов}} = \\frac{3}{4}$$\n",
    "Поскольку из 4 положительных твитов 3 содержали слово awesome.\n",
    "\n",
    "Очевидно, что вероятность не встретить слово awesome в положительном твите равна:\n",
    "$$\\Pr(F_1 = 0 \\mid C=pos) = 1 - \\Pr(F_1 = 1 \\mid C=pos) = 0.25$$\n",
    "\n",
    "Точно так же производятся остальные вычисления.\n",
    "$$\\Pr(F_2 = 1 \\mid C=pos)$$\n",
    "$$\\Pr(F_1 = 1 \\mid C=neg)$$\n",
    "$$\\Pr(F_2 = 1 \\mid C=neg)$$\n",
    "\n",
    "Для полноты картины вычислим свидетельство, чтобы узнать истинные вероятности. Для двух конкретных щначений $F_1$ и $F_2$ свидел=тельство вычисляет ся так:\n",
    "$$\\Pr(F_1, F_2) = \\Pr(F_1, F_2 \\mid C=pos)\\Pr(C=pos) + \\Pr(F_1, F_2 \\mid C=neg)\\Pr(C=neg).$$\n",
    "\n",
    "Пока всё хорошо. При классификации тривиальных твитов метки, похоже вычисляются корректно. \n",
    "\n",
    "Но как быть со словами не встречавшихся в тренировочном корпусе? Ведь всем новым словам будет присвоена нулевая вероятность.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Учет ранее не встречавшихся слов\n",
    "\n",
    "Ранее мы вычисляли не истинные вероятности а лишь грубые приближения к ним. Мы предполагали, что тренировочный корпус содержит полную информацию об истинных вероятностях.\n",
    "\n",
    "Но это не так!\n",
    "\n",
    "Очевидно, что 6 твитов не дадут всю информацию о каждом из когда то написанных твитов. Например, существуют твиты содержащие слово text. Просто мы их не видели. Следовательно, наше приближение очень грубое и нужно учитывать это.\n",
    "\n",
    "На практике для этого часто применяется **сглаживание с прибавлением единицы (add-one smoothing)**.\n",
    "Это очень простой приём, заключающийся в прибавлении единицы ко всем вхождениям признака. В его основе лежит предположение, что даже если мы не видели данного слова во всем корпусе, есть шанс , что это случилось только потому, что в нашей выборке таких твитов не оказалось. \n",
    "\n",
    "То есть вместо вычисления:\n",
    "\n",
    "$$\\Pr(F_1 = 1 \\mid C=pos) = \\frac{\\text{число положительных твитов, содержащих слово awesome}}{\\text{число всех положительных твитов}} = \\frac{3}{4} = 0.75$$\n",
    "Мы вычисляем:\n",
    "$$\\Pr(F_1 = 1 \\mid C=pos) = \\frac{3+1}{4+2}=0.67$$\n",
    "\n",
    "Почему в знаменателе прибавлено 2? \n",
    "Потому что всего у нас два признака: вхождения слов awesome и crazy. Поскольку мы прибавляем 1 для каждого признака нужно позаботиться, чтобы получились всё же вероятности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Потеря точности?\n",
    "Еще одна проблема - вещественная арифметика.\n",
    "Но мы можем прологарифмировать:\n",
    "$ \\log(x,y) = \\log(x) +\\log(y)$\n",
    "\n",
    "В применении к нашему случаю:\n",
    "$$\\log(\\Pr(C)\\times\\Pr(F_1 \\mid C)\\times\\Pr(F_2 \\mid C)) = \\log\\Pr(C) + \\log\\Pr(F_1 \\mid C) + \\log\\Pr(F_2 \\mid C)$$\n",
    "\n",
    "Вероятность лежит в интервале от 0 до 1, значит её логарифм лежит в интервале от $-\\inf$ до 0. Но по прежнему, чем больше число, тем точнее определен класс, только числа теперь отрицательны.\n",
    "\n",
    "Но проблема еще есть: в числителе дроби нет никакого логарифма, а есть лишь произведение вероятностей. К счастью, фактические значения вероятностей нам неизвестны, а нужно знать у какого класса максимальная апостериорная вероятность.\n",
    "\n",
    "И тут нам повезло, потому что если верно, что $\\Pr(C=pos\\mid F_1, F_2) > \\Pr(C=neg \\mid F_1, F_2)$, то верно и то, что $\\log\\Pr(C=pos\\mid F_1, F_2) > \\log\\Pr(C=neg \\mid F_1, F_2)$.\n",
    "\n",
    "Кривая монотонно возрастает, поэтому можно воспользоваться формулой:\n",
    "$$C_{best} = \\arg\\max_{c\\in C} \\Pr(C=c)\\times\\Pr(F_1\\mid C=c)\\times\\Pr(F_2\\mid C=c)$$\n",
    "\n",
    "Откуда мы получаем формулу для двух признаков, которая дает наилучший класс даже для образцов, которые мы ранее не видели:\n",
    "$$C_{best} = \\arg\\max_{c\\in C} (\\log\\Pr(C=c)+\\log\\Pr(F_1\\mid C=c)+\\log\\Pr(F_2\\mid C=c)$$\n",
    "\n",
    "Разумеется, двух признаков маловато, поэтому обобщим на произвольное число признаков:\n",
    "$$C_{best} = \\arg\\max_{c\\in C} (\\log\\Pr(C=c)+\\sum_{k}\\log\\Pr(F_k\\mid C=c)).$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как построить наивный Байесовский классификатор в Python?\n",
    "\n",
    "Scikit-learn имеет 3 модели наивного Байесовского классификатор. \n",
    "* Gaussian: Используется в классификации и предполагает, что атрибуты нормально распределены.\n",
    "* Multinomial: Используется для дискретных атрибутов. (Например, текстовая классификация - можно рассмотреть модель Бернулли, и говорить о том, встречаается ли слово в тексте, или нет, а можно подсчитать, как часто слово встречается в документе. Можно рассматривать как \"число раз, когда атрибут $x_i$ наблюдается.\n",
    "* Bernoulli: Биномиальная модель полезна в том случае, если вектор атрибутов является бинарным. (Пример: классификация текстов, с моделью bag of words, где атрибуты могут быть 0 (слово не встретилось) или 1 (слово встретилось).\n",
    "\n",
    "Ниже рассмотрен пример использования Гауссовской модели наивного Байесовского классификатора.Gaussian model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4]\n"
     ]
    }
   ],
   "source": [
    "#Import Library of Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "#assigning predictor and target variables\n",
    "x= np.array([[-3,7],[1,5], [1,2], [-2,0], [2,3], [-4,0], [-1,1], [1,1], [-2,2], [2,7], [-4,1], [-2,7]])\n",
    "Y = np.array([3, 3, 3, 3, 4, 3, 3, 4, 3, 4, 4, 4])\n",
    "#Create a Gaussian Classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets \n",
    "model.fit(x, Y)\n",
    "\n",
    "#Predict Output \n",
    "predicted= model.predict([[1,2],[3,4]])\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полезные ссылки\n",
    "* https://www.analyticsvidhya.com/blog/2015/09/naive-bayes-explained/\n",
    "* Building Machine Learning Systems with Python (Authors: Willi Richert, Luis Pedro Coelho)\n",
    "* An Introduction to Statistical Learning with Applications in R (Authors: Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani), http://www-bcf.usc.edu/~gareth/ISL/\n",
    "* Machine Learning in Action (Author: Peter Harrington)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбор задачи\n",
    "\n",
    "Пусть дана обучающая выборка с информацией о погоде и соответствующая целевая переменная 'Play'. Сейчас, мы хотим классифицировать, будут ли игроки играть или нет в зависимости от погодных условий.\n",
    "\n",
    "| Weather | Play\n",
    "| :-:  | :-:\n",
    "| Sunny | No\n",
    "| Overcast | Yes\n",
    "| Rainy | Yes\n",
    "| Sunny | Yes\n",
    "| Sunny | Yes\n",
    "| Overcast | Yes\n",
    "| Rainy | No\n",
    "| Rainy | No\n",
    "| Sunny | Yes\n",
    "| Rainy | Yes\n",
    "| Sunny | No\n",
    "| Overcast | Yes\n",
    "| Overcast | Yes\n",
    "| Rainy | No\n",
    "\n",
    "\n",
    "**Шаг 1**: Преобразуем исходные данные в частотную таблицу;\n",
    "\n",
    "Frequency Table:\n",
    "\n",
    "| Weather | No | Yes\n",
    "| :-:  | :-: | :-: \n",
    "| Overcase |  | 4\n",
    "| Rainy | 3 | 2\n",
    "| Sunny | 2 | 3\n",
    "| Grand Total | 5 | 9\n",
    "\n",
    "\n",
    "**Шаг 2**: Создать таблицу правдоподобия (likelihood table);\n",
    "\n",
    "| Weather | No | Yes \n",
    "| :-:  | :-: | :-: \n",
    "| Overcase |  | 4 | = 4/14 | 0.29\n",
    "| Rainy | 3 | 2 | = 5/14 | 0.36\n",
    "| Sunny | 2 | 3 | = 5/14 | 0.36\n",
    "| All | 5 | 9\n",
    "| | =5/14 | =9/14\n",
    "| | 0.36 | 0.64\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Шаг 3 **: Используем предположение о наивности для вычисления апостериорной вероятности для каждого класса. Класс с максимальной апостериорной вероятностью и является результатом предсказания\n",
    "\n",
    "\n",
    "** Проблема: Является ли истинным утверждение, что игроки будут играть, если погода солнечная (Sunny)**\n",
    "\n",
    "Мы можем решить, исходя из предложенного на лекции метода вычисления апостериорной вероятности.\n",
    "\n",
    "$$ \\Pr(Yes \\mid Sunny) = \\frac{\\Pr(Sunny \\mid Yes) \\times \\Pr(Yes)}{\\Pr(Sunny)} \\sim \\Pr(Sunny \\mid Yes) \\times \\Pr(Yes)$$\n",
    "\n",
    "Поскольку, $$\\Pr(Sunny \\mid Yes) = \\frac{3}{9} = 0.33,$$ $$\\Pr(Sunny) = \\frac{5}{14}=0.36,$$ $$\\Pr(Yes) = \\frac{9}{14}=0.64$$.\n",
    "Итак, $$\\Pr(Yes \\mid Sunny)=\\frac{0.33\\times 0.64}{0.36} = 0.60.$$\n",
    "\n",
    "Правдоподобие: $$\\Pr(Yes \\mid Sunny)=\\frac{0.33\\times 0.64} = 0.2112$$\n",
    "\n",
    "$$ \\Pr(No \\mid Sunny) = \\frac{\\Pr(Sunny \\mid No) \\times \\Pr(No)}{\\Pr(Sunny)} \\sim \\Pr(Sunny \\mid No) \\times \\Pr(No)$$\n",
    "Поскольку, $$\\Pr(Sunny \\mid No) = \\frac{2}{5} = 0.4,$$ $$\\Pr(Sunny) = \\frac{5}{14}=0.36,$$ $$\\Pr(No) = \\frac{5}{14}=0.36.$$\n",
    "Итак, $$\\Pr(No \\mid Sunny)=\\frac{0.4 \\times 0.36}{0.36} = 0.60$$\n",
    "\n",
    "Правдоподобие: $$\\Pr(No \\mid Sunny)=\\frac{0.33\\times 0.64} = 0.144$$\n",
    "\n",
    "Правдоподобие (равно как и вероятность $\\Pr(Yes \\mid Sunny)$) больше по сравнению с правдоподобием (или вероятностью  $\\Pr(No \\mid Sunny)$).\n",
    "\n",
    "**Ответ: ** игра состоится.\n",
    "\n",
    "Наивный Байесовский алгоритм имеет похожий метод для предсказания вероятности разных классов, который базируется на различных атрибутах. Алгоритм широко используется в классификации текстов и с проблемами, имеющими несколько классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
